{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vietnamese-boutique",
   "metadata": {},
   "source": [
    "# Dry Bean Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-empty",
   "metadata": {},
   "source": [
    "## Work specification\n",
    "\n",
    "The main purpose of this project is to test and compare **Supervised Learning** models.\n",
    "\n",
    "We are given a dataset which contains various features regarding dry beans. Our goal is to develop a model which will take in features of data beans and in turn will preidct whether a given bean's species type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-complex",
   "metadata": {},
   "source": [
    "## Group\n",
    "\n",
    "- Eduardo Correia - up201806433\n",
    "- João Cardoso - up201806531\n",
    "- Ricardo Fontão - up201806317"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-million",
   "metadata": {},
   "source": [
    "# Used libraries\n",
    "\n",
    "* [scikit-learn](https://scikit-learn.org/)\n",
    "* [pandas](https://pandas.pydata.org/)\n",
    "* [seaborn](https://seaborn.pydata.org/)\n",
    "* [matplotlib](https://matplotlib.org/)\n",
    "* [imblearn](https://imbalanced-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-silicon",
   "metadata": {},
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "bean_data = pd.read_excel(os.getcwd() + \"/../input/Dry_Bean_Dataset.xls\")\n",
    "bean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "bean_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-pearl",
   "metadata": {},
   "source": [
    "### Attribute Information\n",
    "\n",
    "1) **Area (A):** The area of a bean zone and the number of pixels within its boundaries.  \n",
    "2) **Perimeter (P):** Bean circumference is defined as the length of its border.  \n",
    "3) **Major axis length (L):** The distance between the ends of the longest line that can be drawn from a bean.  \n",
    "4) **Minor axis length (l):** The longest line that can be drawn from the bean while standing perpendicular to the main axis.  \n",
    "5) **Aspect ratio (K):** Defines the relationship between L and l.  \n",
    "6) **Eccentricity (Ec):** Eccentricity of the ellipse having the same moments as the region.  \n",
    "7) **Convex area (C):** Number of pixels in the smallest convex polygon that can contain the area of a bean seed.  \n",
    "8) **Equivalent diameter (Ed):** The diameter of a circle having the same area as a bean seed area.  \n",
    "9) **Extent (Ex):** The ratio of the pixels in the bounding box to the bean area.  \n",
    "10) **Solidity (S):** Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.  \n",
    "11) **Roundness (R):** Calculated with the following formula: (4piA)/(P^2)  \n",
    "12) **Compactness (CO):** Measures the roundness of an object: Ed/L  \n",
    "13) **ShapeFactor1 (SF1)**  \n",
    "14) **ShapeFactor2 (SF2)**  \n",
    "15) **ShapeFactor3 (SF3)**  \n",
    "16) **ShapeFactor4 (SF4)**   \n",
    "17) **Class (Seker, Barbunya, Bombay, Cali, Dermosan, Horoz and Sira**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-princess",
   "metadata": {},
   "source": [
    "First we check for missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "bean_data.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-reservoir",
   "metadata": {},
   "source": [
    "No missing values were found. Next we create a plot of the dataset with a color for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line tells the notebook to show plots inside of the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "# sb.pairplot(bean_data.sample(100), hue='Class')\n",
    "# ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-halloween",
   "metadata": {},
   "source": [
    "![Big plot](out.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-healing",
   "metadata": {},
   "source": [
    "## Removing outliers\n",
    "\n",
    "As can be seen in the graph above, there are clearly some outliers in our dataset.\n",
    "\n",
    "Since it's not mentioned in the initial problem's statement if these outliers are to be expected or not, we decided not to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-forth",
   "metadata": {},
   "source": [
    "## Checking for the amount of each bean in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5))\n",
    "\n",
    "sb.countplot(x = bean_data[\"Class\"])\n",
    "plt.title(\"Number of beans per type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-uniform",
   "metadata": {},
   "source": [
    "As it is noticeable, there's a huge discrepancy between the least (Bombay) and most (Dermason) common bean species, with a difference of a factor of 6. We will take this into account when choosing the scoring function used to compare the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-chuck",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Our next step is to create a correlation matrix to compare how each feautre correlates to eachother and to the Class label. For that we need to encode the Class label so that the correlation can be calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(bean_data['Class'])\n",
    "bean_data['Class'] = le.transform(bean_data['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = bean_data.corr()\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title('Correlation Heatmap of Beans Dataset')\n",
    "a = sb.heatmap(corr_matrix, square=True, annot=True, fmt='.2f', linecolor='black')\n",
    "a.set_xticklabels(a.get_xticklabels(), rotation=30)\n",
    "a.set_yticklabels(a.get_yticklabels(), rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-realtor",
   "metadata": {},
   "source": [
    "From this correlation matrix we can exctract features that are strongly correlated with eachother. Since we have a mirrored matrix the analysis can be done by just exctracting the upper matrix triangle and searching for values with an absolute value of more than 0.9 which is our criteria for correlated features.   \n",
    "\n",
    "The first set of features correlated are:\n",
    "* Area\n",
    "* Perimeter\n",
    "* MajorAxisLength\n",
    "* MinorAxisLength\n",
    "* ConvexArea\n",
    "* EquivDiameter\n",
    "* ShapeFactor1 -> even though this feature only has high correlation(>0.9) with MinorAxisLength it presents >0.85 correlation with all other features here presented so we'll include it here\n",
    "\n",
    "\n",
    "The feature to remove is the one that presents the highest correlation with the Class label which in this case is the **Perimeter**.\n",
    "\n",
    "The second set of correlated features are:\n",
    "* ShapeFactor3\n",
    "* Compactness\n",
    "* Eccentricity\n",
    "* AspectRation\n",
    "\n",
    "From this set of features the retained one is **Eccentricity**.\n",
    "\n",
    "So the following features will be dropped:\n",
    "* ShapeFactor3\n",
    "* Compactness\n",
    "* AspectRation\n",
    "* Area\n",
    "* MajorAxisLength\n",
    "* MinorAxisLength\n",
    "* ConvexArea\n",
    "* EquivDiameter\n",
    "* ShapeFactor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_bean_data = bean_data\n",
    "\n",
    "original_bean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "bean_data = bean_data.drop(['ShapeFactor3', \n",
    "                            'Compactness',\n",
    "                            'AspectRation',\n",
    "                            'Area',\n",
    "                            'MajorAxisLength',\n",
    "                            'MinorAxisLength',\n",
    "                            'ConvexArea',\n",
    "                            'EquivDiameter',\n",
    "                            'ShapeFactor1'], axis = 1)\n",
    "\n",
    "bean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-tribe",
   "metadata": {},
   "source": [
    "The function below is used to retrieve the inputs and outputs from the dataset provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(dataset, scaler=None):\n",
    "    X = dataset.drop('Class', axis=1)\n",
    "    y = dataset['Class']\n",
    "    \n",
    "    if scaler != None:\n",
    "        scaler = scaler.fit(X)\n",
    "        X = scaler.transform(X)\n",
    "\n",
    "    return X, y\n",
    "        \n",
    "X, y = get_X_y(bean_data)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-carrier",
   "metadata": {},
   "source": [
    "# Cross validation and parameter tuning\n",
    "## Auxiliary function to perform parameter tuning with cross validation\n",
    "\n",
    "For parameter selection we use GridSearchCV and for oversampling we use imblearn's SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def tune_model(dataset, model_instance, parameter_grid, cross_validation=StratifiedKFold(n_splits=10), scaler=None, oversample=False): \n",
    "    X, y = get_X_y(dataset, scaler)\n",
    "    \n",
    "    if oversample:\n",
    "        steps = [('sampling', SMOTE()), ('model', model_instance)]\n",
    "        model_instance = Pipeline(steps=steps)\n",
    "\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model_instance,\n",
    "        param_grid=parameter_grid,\n",
    "        cv=cross_validation,\n",
    "        scoring=\"f1_weighted\"\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X, y)\n",
    "    print('Best score: {}'.format(grid_search.best_score_))\n",
    "    print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "    grid_search.best_estimator_\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-exclusive",
   "metadata": {},
   "source": [
    "# Time measuring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "def measure_time(dataset, model_instance, params, scaler=None, oversample=False):\n",
    "    X, y = get_X_y(dataset, scaler)\n",
    "\n",
    "    if oversample:\n",
    "        steps = [('sampling', SMOTE()), ('model', model_instance)]\n",
    "        model_instance = Pipeline(steps=steps)\n",
    "    model_instance.set_params(**params)\n",
    "\n",
    "    (X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "    \n",
    "    start = time.time()\n",
    "    model_instance.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-aside",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-dietary",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "parameter_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': range(1, 7),\n",
    "    'max_features': range(1, 7)\n",
    "}\n",
    "\n",
    "# No oversampling / No feature selection\n",
    "dt_original = tune_model(original_bean_data, DecisionTreeClassifier(), parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-tracy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# No oversampling / Feature selection\n",
    "dt = tune_model(bean_data, DecisionTreeClassifier(), parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-douglas",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__splitter': ['best', 'random'],\n",
    "    'model__max_depth': range(1, 7),\n",
    "    'model__max_features': range(1, 7)\n",
    "}\n",
    "\n",
    "# Oversampling / Feature Selection\n",
    "dt_os_fs = tune_model(bean_data, DecisionTreeClassifier(), parameter_grid, oversample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-blackberry",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "The SVM algorithm expects the data to be standardized, so we use the *Sklearn StandardScaler* to standardize our data. If this is not performed prior to training the model, the efficiency will be all over the place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_X_y(bean_data)\n",
    "\n",
    "# Without standardizing the data:\n",
    "svc = SVC()\n",
    "\n",
    "# cross_val_score returns a list of the scores, which we can visualize\n",
    "# to get a reasonable estimate of our classifier's performance\n",
    "cv_scores = cross_val_score(svc, X, y, cv=10)\n",
    "\n",
    "plt.hist(cv_scores)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data:\n",
    "standardized_X, y = get_X_y(bean_data, scaler = StandardScaler())\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "# cross_val_score returns a list of the scores, which we can visualize\n",
    "# to get a reasonable estimate of our classifier's performance\n",
    "cv_scores = cross_val_score(svc, standardized_X, y, cv=10)\n",
    "plt.hist(cv_scores)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-sword",
   "metadata": {},
   "source": [
    "By comparing both histograms, it can be easily concluded that the standardization is really necessary and produces better and more consistent results.\n",
    "\n",
    "Still the cross validation scores vary a lot based on the training data chosen. Therefore we should do some parameter tuning to see what the best parameters are for our dataset that don't overfit the data. This can be achieved by a GridSearch. This will be addressed below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'C': [1, 10, 50], \n",
    "    'gamma': [0.001, 0.0001],\n",
    "    # 'kernel': ['linear', 'poly', 'rbf']\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "# No oversampling / No feature selection\n",
    "svc_original = tune_model(original_bean_data, SVC(), parameter_grid, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No oversampling / Feature selection\n",
    "svc = tune_model(bean_data, SVC(), parameter_grid, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'model__C': [1, 10, 50], \n",
    "    'model__gamma': [0.001, 0.0001],\n",
    "    # 'kernel': ['linear', 'poly', 'rbf']\n",
    "    'model__kernel': ['linear', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Oversampling / Feature Selection\n",
    "svc_os_fs = tune_model(bean_data, SVC(), parameter_grid, scaler=StandardScaler(), oversample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-translator",
   "metadata": {},
   "source": [
    "# K-nearest neighbours (KNN)\n",
    "Just like the SVM model, the KNN model also requires the data to be standardised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without standardizing the data\n",
    "from sklearn import neighbors\n",
    "\n",
    "X, y = get_X_y(bean_data)\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "\n",
    "# cross_val_score returns a list of the scores, which we can visualize\n",
    "# to get a reasonable estimate of our classifier's performance\n",
    "cv_scores = cross_val_score(knn, X, y, cv=10)\n",
    "plt.hist(cv_scores)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-holder",
   "metadata": {},
   "source": [
    "Without standardizing the data the results are simply bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data\n",
    "standardized_X, y = get_X_y(bean_data, scaler=StandardScaler())\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "\n",
    "cv_scores = cross_val_score(knn, standardized_X, y, cv=10)\n",
    "plt.hist(cv_scores)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-donna",
   "metadata": {},
   "source": [
    "Now we can run the model some times to see its efficiency with the default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-match",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_grid =  {\n",
    "    'n_neighbors':[4,5,6,7,10,15],\n",
    "    'leaf_size':[5, 10, 15, 20, 50, 100],\n",
    "    'n_jobs':[-1],\n",
    "    'algorithm':['auto']\n",
    "}\n",
    "\n",
    "# No oversampling / No feature selection\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn_original = tune_model(original_bean_data, knn, parameter_grid, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-allah",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# No oversampling / Feature selection\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn = tune_model(bean_data, knn, parameter_grid, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'model__n_neighbors':[4,5,6,7,10,15],\n",
    "    'model__leaf_size':[5, 10, 15, 20, 50, 100],\n",
    "    'model__n_jobs':[-1],\n",
    "    'model__algorithm':['auto']\n",
    "}\n",
    "\n",
    "# Oversampling / Feature Selection\n",
    "knn_os_fs = tune_model(bean_data, neighbors.KNeighborsClassifier(), parameter_grid, scaler=StandardScaler(), oversample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-grammar",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "parameter_grid = {}\n",
    "\n",
    "# No oversampling / No feature selection\n",
    "nb_original = tune_model(original_bean_data, GaussianNB(), parameter_grid, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No oversampling / Feature selection\n",
    "nb = tune_model(bean_data, GaussianNB(), parameter_grid, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {}\n",
    "\n",
    "# Oversampling / Feature Selection\n",
    "nb_os_fs = tune_model(bean_data, GaussianNB(), parameter_grid, scaler=StandardScaler(), oversample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-slide",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-tobacco",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# parameter_grid = {\n",
    "#     'n_estimators': [100,200],\n",
    "#     'max_depth': [8, 9, 10],\n",
    "#     'n_jobs': [-1], #Use all cores\n",
    "#     'max_features': ['auto', 'sqrt'],\n",
    "#     'criterion': ['gini', 'entropy']\n",
    "# }\n",
    "\n",
    "parameter_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'n_jobs': [-1], #Use all cores\n",
    "    'max_features': ['auto'],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# No oversampling / No feature selection\n",
    "rfc_original = tune_model(original_bean_data, RandomForestClassifier(), parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-isolation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# No oversampling / Feature selection\n",
    "rfc = tune_model(bean_data, RandomForestClassifier(), parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-price",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'model__n_estimators': [10, 50, 100, 200],\n",
    "    'model__max_depth': [5, 10, 15],\n",
    "    'model__n_jobs': [-1], #Use all cores\n",
    "    'model__max_features': ['auto'],\n",
    "    'model__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Oversampling / Feature Selection\n",
    "rfc_os_fs = tune_model(bean_data, RandomForestClassifier(), parameter_grid, oversample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-mount",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    \"Decision Tree\" : [dt_original, dt, dt_os_fs],\n",
    "    \"SVC\" : [svc_original, svc, svc_os_fs],\n",
    "    \"K-nearest Neighbours\" : [knn_original, knn, knn_os_fs],\n",
    "    \"Naive Bayes\" : [nb_original, nb, nb_os_fs],\n",
    "    \"Random Forest\" : [rfc_original, rfc, rfc_os_fs]\n",
    "}\n",
    "\n",
    "labels = [\"No oversampling/No feature selection\",\"No oversampling/Feature selection\", \"Oversampling/Feature selection\"]\n",
    "\n",
    "ind = np.arange(5)\n",
    "\n",
    "plt.figure(figsize=(10,9))\n",
    "plt.bar(ind, [i[0].best_score_ for i in scores.values()], 0.2)\n",
    "ax = plt.bar(ind + 0.2, [i[1].best_score_ for i in scores.values()], 0.2)\n",
    "ax = plt.bar(ind + 0.4, [i[2].best_score_ for i in scores.values()], 0.2)\n",
    "plt.xticks(ind, scores.keys())\n",
    "plt.legend(labels,loc=1)\n",
    "plt.ylim(0.7, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-flower",
   "metadata": {},
   "source": [
    "# Analysing times to train\n",
    "\n",
    "Below we present a plot with the times to train each model one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = {\n",
    "    \"Decision Tree\" : [\n",
    "        measure_time(original_bean_data, DecisionTreeClassifier(), dt_original.best_params_),\n",
    "        measure_time(bean_data, DecisionTreeClassifier(), dt.best_params_),\n",
    "        measure_time(bean_data, DecisionTreeClassifier(), dt_os_fs.best_params_, oversample=True)\n",
    "    ],\n",
    "    \"SVC\" : [\n",
    "        measure_time(original_bean_data, SVC(), svc_original.best_params_, scaler=StandardScaler()),\n",
    "        measure_time(bean_data, SVC(), svc.best_params_, scaler=StandardScaler()),\n",
    "        measure_time(bean_data, SVC(), svc_os_fs.best_params_, oversample=True, scaler=StandardScaler())\n",
    "    ],\n",
    "    \"K-nearest Neighbours\" : [\n",
    "        measure_time(original_bean_data, neighbors.KNeighborsClassifier(), knn_original.best_params_, scaler=StandardScaler()),\n",
    "        measure_time(bean_data, neighbors.KNeighborsClassifier(), knn.best_params_, scaler=StandardScaler()),\n",
    "        measure_time(bean_data, neighbors.KNeighborsClassifier(), knn_os_fs.best_params_, oversample=True, scaler=StandardScaler())\n",
    "    ],\n",
    "    \"Naive Bayes\" : [\n",
    "        measure_time(original_bean_data, GaussianNB(), nb_original.best_params_, scaler=StandardScaler()),\n",
    "        measure_time(bean_data, GaussianNB(), nb.best_params_, scaler=StandardScaler()),\n",
    "        measure_time(bean_data, GaussianNB(), nb_os_fs.best_params_, oversample=True, scaler=StandardScaler())\n",
    "    ],\n",
    "    \"Random Forest\" : [\n",
    "        measure_time(original_bean_data, RandomForestClassifier(), rfc_original.best_params_),\n",
    "        measure_time(bean_data, RandomForestClassifier(), rfc.best_params_),\n",
    "        measure_time(bean_data, RandomForestClassifier(), rfc_os_fs.best_params_, oversample=True)\n",
    "    ]\n",
    "}\n",
    "\n",
    "labels = [\"No oversampling/No feature selection\",\"No oversampling/Feature selection\", \"Oversampling/Feature selection\"]\n",
    "\n",
    "ind = np.arange(5)\n",
    "\n",
    "plt.figure(figsize=(10,9))\n",
    "plt.bar(ind, [i[0] for i in times.values()], 0.2)\n",
    "ax = plt.bar(ind + 0.2, [i[1] for i in times.values()], 0.2)\n",
    "ax = plt.bar(ind + 0.4, [i[2] for i in times.values()], 0.2)\n",
    "plt.xticks(ind, times.keys())\n",
    "plt.legend(labels,loc=1)\n",
    "\n",
    "# plt.ylim(0.7, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-incident",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The proposed work was to test and compare different Supervised Machine Learning models for classification of the Dry beans dataset. The tested models were **Decision Tree**, **Support Vector Machines**, **K-nearest Neighbours**, **Naive Bayes** and **Random Forest**. \n",
    "\n",
    "After some exploratory data analysis we decided to drop some features based on their correlation with each other. This proved to be only effective in the **Naive Bayes** and **Random Forest Classifiers**. \n",
    "\n",
    "To evaluate each model and choose the best parameters for each one, we used SKLearn's GridSearchCV to test different set of parameters. To score the models we used **f1 wighted score**. We also tried combining oversampling with and without feature selection. Looking at the benchmarks we can conclude that oversampling does not improve the scores on our models while increasing significantly the training time. \n",
    "\n",
    "In terms of scoring, it can be concluded that the best models for our classification problem is the **Support Vector Machine**, followed closely by the **K-nearest Neighbors**. However when we take a look at the time needed to train each model, the **Support Vector Machine** takes much longer than **K-nearest Neighbours**, making **K-nearest neighbours** the best model overall. This appears to be related to the fact that **K-nearest Neighbours** can be trained with the flag n_jobs=-1 which makes it use all the cores in the CPU while **Support Vector Machine** does not support this option."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
