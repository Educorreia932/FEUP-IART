{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accompanied-artwork",
   "metadata": {},
   "source": [
    "# 7.2 Restaurant Reviews Dataset\n",
    "\n",
    "The restaurant reviews dataset includes 1000 single-line English reviews on restaurants, associated\n",
    "with a polarity (1 = positive review; 0 = negative review). The text included in reviews can be seen\n",
    "as noisy, in the sense that not every token corresponds to a word in English or a punctuation mark.\n",
    "In this Notebook, we will develop sentiment analysis modelsfor predicting the polarity of restaurant\n",
    "reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-pocket",
   "metadata": {},
   "source": [
    "**b)** Use the pandas library to Import the restaurant reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aboriginal-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Importing the dataset\n",
    "df = pd.read_csv(os.getcwd() + \"/Restaurant_Reviews.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-runner",
   "metadata": {},
   "source": [
    "**c)** Using the regular expressions library (re), perform some simple cleanup on the text. For\n",
    "example, you may consider only alphabetic character sequences, and convert the whole text\n",
    "to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interested-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "reviews = []\n",
    "\n",
    "for i in range(0, len(df.index)):\n",
    "    # Fet review and remove non alpha chars, to lower-case and tokenize\n",
    "    reviews.append(re.sub('[^a-zA-Z]', ' ', df['Review'][i]).lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-possible",
   "metadata": {},
   "source": [
    "**d)** Tokenize the text, use NLTK’s Porter stemmer to stem the obtained tokens, and remove stop\n",
    "words using NLTK’s stop word list for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "artificial-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "\n",
    "for review in reviews:\n",
    "    # Stemming and stop word removal\n",
    "    review = ' '.join([ps.stem(word) for word in review if not word in set(stopwords.words('english'))])\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-syria",
   "metadata": {},
   "source": [
    "**e)** From the cleaned up and tokenized corpus, create bag-of-words features, using sklearn’s\n",
    "CountVectorizer. Now you should have obtained a structured dataset, where each\n",
    "restaurant review is represented by a list of 0’s and 1’s with the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "automotive-strip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1500) (1000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1500)\n",
    "x = vectorizer.fit_transform(corpus).toarray()\n",
    "y = df.iloc[:,-1].values\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-sleep",
   "metadata": {},
   "source": [
    "**f)** Split the data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "possible-handling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1500) (800,)\n",
      "(200, 1500) (200,)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-catholic",
   "metadata": {},
   "source": [
    "**g)** Try to fit a Naïve Bayes classifier to the training set, and predict its test set results. Analyse\n",
    "the confusion matrix and the classification scores (accuracy, precision, recall, F1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "protective-advisory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0\n",
      " 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
      " 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1]\n",
      "[[55 42]\n",
      " [12 91]]\n",
      "Accuracy:  0.73\n",
      "Precision:  0.6842105263157895\n",
      "Recall:  0.883495145631068\n",
      "F1:  0.7711864406779663\n"
     ]
    }
   ],
   "source": [
    "# Fit Naive Bayes to the training set\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# Predict test set results\n",
    "\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "# Generate metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# accuracy\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# precision\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "\n",
    "# recall\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "\n",
    "# f1\n",
    "print('F1: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-contest",
   "metadata": {},
   "source": [
    "**h)** Try out the model by prompting the user to input a restaurant review and predicting its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test\n",
    "\n",
    "rev = input(\"Enter review: \")\n",
    "rev = re.sub('[^a-zA-Z]', ' ', rev).lower().split()\n",
    "rev = ' '.join([ps.stem(w) for w in rev])\n",
    "X = vectorizer.transform([rev]).toarray()\n",
    "\n",
    "if(classifier.predict(X) == [1]):\n",
    "    print('positive review (+)')\n",
    "    \n",
    "else:\n",
    "    print('negative review (-)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-triangle",
   "metadata": {},
   "source": [
    "**i)** Experiment with other classifiers and see if you can improve on the performance of the\n",
    "classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-destruction",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-season",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-disaster",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "classifier = Perceptron()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-passing",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-amplifier",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred))\n",
    "print('Recall: ', recall_score(y_test, y_pred))\n",
    "print('F1: ', f1_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
